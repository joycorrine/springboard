{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone 2 - Abalone Age Prediction\n",
    "### Modeling\n",
    "**Context**:\n",
    "\n",
    "The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.\n",
    "\n",
    "_Credit: https://www.kaggle.com/rodolfomendes/abalone-dataset_\n",
    "\n",
    "**Goal**: The goal of this capstone project is to build a regression model that can predict the age of an abalone shell by accurately predicting its ring count.\n",
    "\n",
    "\n",
    "**Pre-processing & Training Data Development Objective**: Build two to three different models and identify the best one to predict the age of an abalone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve, cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import abalone dataset\n",
    "abalone_data = pd.read_csv('/Users/joyopsvig/github/springboard/2-CapstoneAbalone/Notebooks/abaloneDW_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Age</th>\n",
       "      <th>F</th>\n",
       "      <th>I</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell weight   Age  F  I  M  \n",
       "0         0.150  16.5  0  0  1  \n",
       "1         0.070   8.5  0  0  1  \n",
       "2         0.210  10.5  1  0  0  \n",
       "3         0.155  11.5  0  0  1  \n",
       "4         0.055   8.5  0  1  0  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One hot encode the 'Sex' column since it is categorical\n",
    "one_hot = pd.get_dummies(abalone_data['Sex'])\n",
    "\n",
    "# Drop 'Sex' column as it is now encoded\n",
    "abalone_data = abalone_data.drop('Sex',axis = 1)\n",
    "\n",
    "# Join the encoded df\n",
    "abalone_data = abalone_data.join(one_hot)\n",
    "\n",
    "#Confirm Sex is one hot encoded\n",
    "abalone_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop response variable\n",
    "X = abalone_data.drop('Age', axis = 1)\n",
    "y = abalone_data['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform data so that it has a mean of 0 and std of 1\n",
    "standardScale = StandardScaler()\n",
    "standardScale.fit_transform(X)\n",
    "\n",
    "#Split data in to train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "\n",
    "_In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. Wikipedia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the linear regression model and fit it to training data\n",
    "model_lin = linear_model.LinearRegression()\n",
    "model_lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 4.69\n",
      "Root mean squared error (RMSE): 2.16\n",
      "Coefficient of determination (R^2): 0.55\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "y_pred_train_lin = model_lin.predict(X_train)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_train, y_pred_train_lin)) #Mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_train, y_pred_train_lin)))\n",
    "print('Coefficient of determination (R^2): %.2f' #Coefficient of determination\n",
    "      % r2_score(y_train, y_pred_train_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 5.18\n",
      "Root mean squared error (RMSE): 2.28\n",
      "Coefficient of determination (R^2): 0.51\n"
     ]
    }
   ],
   "source": [
    "#Apply model to test set\n",
    "y_pred_test_lin = model_lin.predict(X_test)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_test, y_pred_test_lin)) #mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_test, y_pred_test_lin)))\n",
    "print('Coefficient of determination (R^2): %.2f'\n",
    "      % r2_score(y_test, y_pred_test_lin)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression Model\n",
    "\n",
    "_Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where linearly independent variables are highly correlated. Wikipedia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the ridge regression model and fit it to training data\n",
    "model_ridge = Ridge()\n",
    "model_ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 4.73\n",
      "Root mean squared error (RMSE): 2.17\n",
      "Coefficient of determination (R^2): 0.54\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "y_pred_train_ridge = model_ridge.predict(X_train)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_train, y_pred_train_ridge)) #Mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_train, y_pred_train_ridge)))\n",
    "print('Coefficient of determination (R^2): %.2f' #Coefficient of determination\n",
    "      % r2_score(y_train, y_pred_train_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 5.22\n",
      "Root mean squared error (RMSE): 2.28\n",
      "Coefficient of determination (R^2): 0.50\n"
     ]
    }
   ],
   "source": [
    "#Apply model to test set\n",
    "y_pred_test_ridge = model_ridge.predict(X_test)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_test, y_pred_test_ridge)) #mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_test, y_pred_test_ridge)))\n",
    "print('Coefficient of determination (R^2): %.2f'\n",
    "      % r2_score(y_test, y_pred_test_ridge)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model\n",
    "\n",
    "_Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For regression tasks, the mean or average prediction of the individual trees is returned. Wikipedia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the random forest model and fit it to training data\n",
    "model_rf = RandomForestRegressor()\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 0.66\n",
      "Root mean squared error (RMSE): 0.81\n",
      "Coefficient of determination (R^2): 0.94\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "y_pred_train_rf = model_rf.predict(X_train)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_train, y_pred_train_rf)) #Mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_train, y_pred_train_rf)))\n",
    "print('Coefficient of determination (R^2): %.2f' #Coefficient of determination\n",
    "      % r2_score(y_train, y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 4.95\n",
      "Root mean squared error (RMSE): 2.23\n",
      "Coefficient of determination (R^2): 0.53\n"
     ]
    }
   ],
   "source": [
    "#Apply model to test set\n",
    "y_pred_test_rf = model_rf.predict(X_test)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_test, y_pred_test_rf)) #mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_test, y_pred_test_rf)))\n",
    "print('Coefficient of determination (R^2): %.2f'\n",
    "      % r2_score(y_test, y_pred_test_rf)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM) / Support Vector Regression (SVR)\n",
    "\n",
    "_Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier. Wikipedia_ \n",
    "\n",
    "_Support Vector Regression is a supervised learning algorithm that is used to predict discrete values. Support Vector Regression uses the same principle as the SVMs. The basic idea behind SVR is to find the best fit line. In SVR, the best fit line is the hyperplane that has the maximum number of points. Towards Data Science_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the SVR model and fit it to training data\n",
    "model_svr = SVR()\n",
    "model_svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 5.05\n",
      "Root mean squared error (RMSE): 2.25\n",
      "Coefficient of determination (R^2): 0.51\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "y_pred_train_svr = model_svr.predict(X_train)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_train, y_pred_train_svr)) #Mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_train, y_pred_train_svr)))\n",
    "print('Coefficient of determination (R^2): %.2f' #Coefficient of determination\n",
    "      % r2_score(y_train, y_pred_train_svr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 5.69\n",
      "Root mean squared error (RMSE): 2.39\n",
      "Coefficient of determination (R^2): 0.46\n"
     ]
    }
   ],
   "source": [
    "#Apply model to test set\n",
    "y_pred_test_svr = model_svr.predict(X_test)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_test, y_pred_test_svr)) #mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_test, y_pred_test_svr)))\n",
    "print('Coefficient of determination (R^2): %.2f'\n",
    "      % r2_score(y_test, y_pred_test_svr)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Model\n",
    "\n",
    "_Gradient boosting is a machine learning technique used in regression and classification tasks, among others. It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees.[1][2] When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. Wikipedia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the gradient boost model and fit it to training data\n",
    "model_gb = GradientBoostingRegressor()\n",
    "model_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 3.42\n",
      "Root mean squared error (RMSE): 1.85\n",
      "Coefficient of determination (R^2): 0.67\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "y_pred_train_gb = model_gb.predict(X_train)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_train, y_pred_train_gb)) #Mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_train, y_pred_train_gb)))\n",
    "print('Coefficient of determination (R^2): %.2f' #Coefficient of determination\n",
    "      % r2_score(y_train, y_pred_train_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 5.04\n",
      "Root mean squared error (RMSE): 2.24\n",
      "Coefficient of determination (R^2): 0.52\n"
     ]
    }
   ],
   "source": [
    "#Apply model to test set\n",
    "y_pred_test_gb = model_gb.predict(X_test)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_test, y_pred_test_gb)) #mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_test, y_pred_test_gb)))\n",
    "print('Coefficient of determination (R^2): %.2f'\n",
    "      % r2_score(y_test, y_pred_test_gb)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors (KNN)\n",
    "\n",
    "_KNN model is popularly used for non-linear regression in Machine Learning. KNN (K Nearest Neighbours) follows an easy implementation approach for non-linear regression in Machine Learning. KNN assumes that the new data point is similar to the existing data points. The new data point is compared to the existing categories and is placed under a relatable category. The average value of the k nearest neighbors is taken as the input in this algorithm. The neighbors in KNN models are given a particular weight that defines their contribution to the average value. Jigsaw Academy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=4)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the KNN model and fit it to training data\n",
    "model_knn = KNeighborsRegressor(n_neighbors = 4)\n",
    "model_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 3.11\n",
      "Root mean squared error (RMSE): 1.76\n",
      "Coefficient of determination (R^2): 0.70\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "y_pred_train_knn = model_knn.predict(X_train)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_train, y_pred_train_knn)) #Mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_train, y_pred_train_knn)))\n",
    "print('Coefficient of determination (R^2): %.2f' #Coefficient of determination\n",
    "      % r2_score(y_train, y_pred_train_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (MSE): 5.36\n",
      "Root mean squared error (RMSE): 2.31\n",
      "Coefficient of determination (R^2): 0.49\n"
     ]
    }
   ],
   "source": [
    "#Apply model to test set\n",
    "y_pred_test_knn = model_knn.predict(X_test)\n",
    "\n",
    "#Evaluate the performance\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(y_test, y_pred_test_knn)) #mean squared error (MSE)\n",
    "print('Root mean squared error (RMSE): %.2f' #Root mean squared error (RMSE)\n",
    "      % sqrt(mean_squared_error(y_test, y_pred_test_knn)))\n",
    "print('Coefficient of determination (R^2): %.2f'\n",
    "      % r2_score(y_test, y_pred_test_knn)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
